---
title: "Text Analyis"
output: github_document
---




# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
    ```{r}
    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
    ```
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

# Timelines and Task


We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 

# Group Question Sets
```{r}
## Questions for analysis: 
## 1. what is the most important charecter based on how much is whas mentioned ? 
## 2. what is the most scariest book based on sentiment analysis ?
## 3. what the top ten used words in exception to stop words ?
## 4. sentiments by books 
## 5. sentiment by popularity based on "https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare" 
# And the sixth question is unique for every student.
```

# Preperation
```{r}

    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")



library(devtools)
library(harrypotter)
library(rebus)
library(tidytext)
library(dplyr)
library(stringr)
library(stringi)
library(ggplot2)
```



#1. Preperation
```{r}

book_tidy = function(name){
  chapter = tibble(text = get(name))%>%
    mutate(chapter = row_number())
  
  sentence = chapter%>%
    unnest_tokens(sentence, text, token = "sentences")%>%
    mutate(sentences = row_number())
  
  df = sentence %>%
    unnest_tokens(word, sentence)
  title = tibble(book = name)
  return(cbind(title, df))
}


names = c("philosophers_stone",
         "chamber_of_secrets",
         "prisoner_of_azkaban",
         "goblet_of_fire",
         "order_of_the_phoenix",
         "half_blood_prince",
         "deathly_hallows")



#
harrypotter = rep(list(tibble()), 7)
whole_series = tibble()

#Save 7 books in a list, each in a tibble and remove the stop words.
for(i in 1:7){
  harrypotter[[i]] = book_tidy(names[i]) %>%
                     anti_join(stop_words)
 
}

for(i in 1:length(names)){
  whole_series = rbind(whole_series, book_tidy(names[i]))%>%
    anti_join(stop_words, by = "word")
}

#The name of main characters was found in website "https://en.wikipedia.org/wiki/List_of_Harry_Potter_characters", the content is simply copy and paste into local .txt document called characters.txt.

characters_txt = readLines("harry_potter_characters.txt")
pattern = "\t" %R% capture(one_or_more(WRD)) %R% SPC %R% capture(one_or_more(WRD))
characters = str_match(str_to_lower(characters_txt), pattern = pattern)%>%
  as_tibble()%>%
  setNames(c("name", "first_name", "last_name"))%>%
  filter(!is.na(name))

name = tibble(name = unique(c(characters$first_name, characters$last_name)))

name_freq = whole_series%>%
  inner_join(name, by = c(word = "name"))%>%
  count(book, word)%>%
  group_by(book)%>%
  top_n(10)%>%
  arrange(desc(n))%>%
  ungroup()%>%
  mutate(word = reorder(word, n))
 

ggplot(name_freq, aes(x = word, y = n, fill = book))+
   geom_col(show.legend = FALSE) +
  facet_wrap(~book, scales = "free") +  
  coord_flip()
 
#As we can see in the plot, Harry is the most important character in the novel. Also the second and third important character is Ron and Hermione.

#Count the name frequency based on chapter
name_chap_freq = whole_series%>%
  inner_join(name, by = c(word = "name"))%>%
  filter(word != "harry")%>%
  count(book, chapter, word)%>%
  group_by(book, chapter)%>%
  top_n(1)%>%
  ungroup()%>%
  mutate(word = reorder(word, n))

ggplot(name_chap_freq, aes(x = chapter, y = n, fill = word))+
   geom_col() +
  facet_wrap(~book, scales = "free_x")

#We can see except for harry, who is the most important character in each chapter based on the frequency of the name.

word_freq = whole_series%>%
  anti_join(name, by = c(word = "name"))%>%
  count(book, word)%>%
  group_by(book)%>%
  top_n(10)%>%
  ungroup()%>%
  mutate(word = reorder(word, n))

ggplot(word_freq, aes(x = word, y = n, fill = book))+
  geom_col(show.legend = F)+
  facet_wrap(~book, scales = "free")+
  coord_flip()



```

# Sentiment analysis
##1. The scarest book
```{r}
emotion = get_sentiments("nrc")
unique(emotion[,2])

total = whole_series%>%
  count(book)%>%
  rename(total_word = n)


scare_emotion = whole_series%>%
  left_join(total, by = "book")%>%
  inner_join(emotion, by = "word")%>%
  count(book, sentiment, total_word)%>%
  ungroup()%>%
  mutate(percent = n/total_word)%>%
  filter(sentiment == "fear")%>%
  arrange(desc(percent))


  ggplot(scare_emotion, aes(x = book, y = percent, fill = book))+
  geom_col(show.legend = F)

#And we can see based on chapter, how does the sentiment become.
  
  score = get_sentiments("afinn")
emotion_in_chap = whole_series%>%
 count(book, chapter, word)%>%
 inner_join(score, by = "word")%>%
  group_by(book, chapter)%>%
  mutate(contribution = sum(score*n)/sum(n))%>%
  ungroup()

  

ggplot(emotion_in_chap, aes(x = chapter, y = contribution, fill = book))+
  geom_col()+
  facet_wrap(~book, scales = "free_x")
  
```

