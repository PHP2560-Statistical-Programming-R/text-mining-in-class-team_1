---
title: "Text Analyis"
output: github_document
---




# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
    ```
    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
    ```
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

# Timelines and Task


We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 

## Package installation
```{r, message=FALSE, warning=FALSE, eval=FALSE}
#install.packages("janeaustenr")  # for book by janeaustenr
#install.packages("gutenbergr")  # for free ebooks
#install.packages("dplyr")  # for data manipulation
#install.packages("stringr")  # for string manupulation
#install.packages("tidytext")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("ggplot2")  # for text stemming
#install.packages("wordcloud") # word-cloud generator 
#install.packages("RColorBrewer") # color palettes
#install.packages("ggrepel") # ggrepel
#install.packages("igraph") # ggrepel
#install.packages("ggraph") # ggrepel
#

if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}
devtools::install_github("bradleyboehmke/harrypotter")
```

## Loading required packages
```{r message=FALSE, warning=FALSE}
library(janeaustenr)
library(gutenbergr)
library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(harrypotter)
library(tidyr)
library(ggrepel)
library(RColorBrewer)
library(igraph)
library(ggraph)


```
## Reusable functions
```{r}
# this function returns ggplot theme definition ~ this will prevent code duplication
uniform_ggplot_theme <- function(base_size = 12, base_family = "sans"){
  theme_grey(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "aliceblue"),
    strip.background = element_rect(fill = "lightgrey", color = "grey", size = 1),
    strip.text = element_text(face = "bold", size = 12, color = "navy"),
    legend.position = "bottom",
    legend.background = element_blank(),
    panel.margin = unit(.5, "lines"),
    panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
  )
}
```

## Merging data from different sources
Let’s start with that, and also use mutate() to annotate a linenumber quantity to keep track of lines in the original format and a chapter (using a regex) to find where all the chapters are.
```{r}

# Fetch Book metadata from theGuardian.com e.g publisher, ranking, sales, author e.t.c 
bookMetadata <- read.csv("https://docs.google.com/spreadsheets/d/1dhxblR1Vl7PbVP_mNhwEa3_lfUWiF__xSODLq1W83CA/export?format=csv&id=1dhxblR1Vl7PbVP_mNhwEa3_lfUWiF__xSODLq1W83CA&gid=0")

# Combine all The Harry Potter books into 1 dataframe and add column which will be used a reference "Title"

book1 <- data_frame(Title="Harry Potter and the Philosopher's Stone", text=philosophers_stone)
book2 <- data_frame(Title="Harry Potter and the Chamber of Secrets", text=chamber_of_secrets)
book3 <- data_frame(Title="Harry Potter and the Prisoner of Azkaban", text=prisoner_of_azkaban)
book4 <- data_frame(Title="Harry Potter and the Goblet of Fire", text=goblet_of_fire)
book5 <- data_frame(Title="Harry Potter and the Order of the Phoenix", text=order_of_the_phoenix)
book6 <- data_frame(Title="Harry Potter and the Half-blood Prince", text=half_blood_prince)
book7 <- data_frame(Title="Harry Potter and the Deathly Hallows", text=deathly_hallows)

# Join BookMetadata and HarryPotter dfs by "Title"
harrypotter_and_metadata <- rbind(book1,book2, book3, book4, book5, book6, book7) %>% inner_join(bookMetadata, by = "Title")%>%
  group_by(Title) %>%
  mutate(Chapter = row_number())%>% # add chapter
     ungroup()

  harrypotterSeries <- harrypotter_and_metadata %>%
 unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(line = row_number()) # add sentences

head(harrypotterSeries)
```

## Cleaning Data
1. Restructure it in the one-token-per-row format, 

```{r}
harrypotter_tokenized <- harrypotterSeries %>%
  unnest_tokens(word, sentence)

harrypotter_tokenized %>%
  select(Title, Chapter, word) %>%
  head()

```

2. Remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as "the", "of", "to",

```{r}
harrypotter_clean_tokens <- harrypotter_tokenized %>% 
   anti_join(stop_words)  #remove stop words like "the"
```

## Quantitative and Sentimental Analysis
1. what is the most important charecter based on how much is whas mentioned 
2. what is the most scariest book based on sentiment analysis ?

```{r}
harrypotter_clean_tokens%>%
  inner_join(get_sentiments("bing"), by = "word") %>% # join sentiment
  group_by(Title)%>% # make each row  abook
  count(sentiment, sort = TRUE)%>% # count sentiment
  filter(sentiment=="negative")  #filter by negative sentiment

# Using bing lexicon, Harry Potter and the Order of the Phoenix is the scariest with about 8k negative sentiments

```

3. What the top ten used words in exception to stop words ?
```{r}
word_count <- harrypotter_clean_tokens %>% 
  count(word, sort = TRUE) %>% 
   mutate(word = reorder(word, n))
head(word_count)
```
 
```{r}
# Visualization of the most common words
word_count_gt_600 <- word_count %>%
  filter(n > 600)



# plot using word cloud
 
  wordcloud(words = word_count_gt_600$word, freq = word_count_gt_600$n, min.freq = 1,
          random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
 
```

## More Sentimental & Quantitative Analysis

1. Which is the most common word pairs (ngrams) in harry potter series 
```{r}
harrypotterSeries_ngrams <- harrypotterSeries %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- harrypotterSeries_ngrams %>%
  count(word1, word2, sort = TRUE)

head(bigram_counts)

bigram_counts%>%
  filter(n > 80)%>%
  ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) +
    geom_tile(alpha = 0.8, color = "white") +
    coord_flip() +
    theme(legend.position = "right") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "first word in pair",
         y = "second word in pair")

```
1. Analyze the network structure of the word pairs (ngrams) in harry potter series 
```{r}
bigram_graph <- bigram_counts %>%
  filter(n > 60) %>%
  graph_from_data_frame()

set.seed(2017)

a <- grid::arrow(type = "closed", length = unit(.10, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "chartreuse", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```


1. Examine how often sentiment-associated words are preceded by “not” or other negating words.

```{r}
# create hp_big_grams object
hp_bigrams_separated  <- harrypotterSeries %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
   count(word1, word2, sort = TRUE)

# AFINN gives a numeric sentiment score for each word, with positive or negative numbers indicating the direction of the sentiment.
AFINN <- get_sentiments("afinn")


negative_words <- c("not", "no", "never", "without")

hp_negated_words  <- hp_bigrams_separated %>%
  filter(word1 %in% negative_words) %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

head(hp_negated_words)

hp_negated_words %>%
  mutate(contribution = nn * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, nn * score, fill = nn * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```


1.What is the frequency distribution of words for each Harry Potter Book
```{r}
 book_words <- harrypotter_clean_tokens %>%
  count(Title, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(Title) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

head(book_words)
ggplot(book_words, aes(n/total, fill = Title)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~Title, ncol = 2, scales = "free_y")

# These plots exhibit similar distributions for all the books, with most of the words occuring rarely and fewer words that occur frequently.
```
3. Does Zipf’s law holds for Harry Potter series? 
```{r}
freq_by_rank <- book_words %>% 
  group_by(Title) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

head(freq_by_rank)
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = Title)) + 
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()

# We see that all 7 novels are similar to each other, and that the relationship between rank and frequency does have negative slope. 

```
3. Does  *power law*  holds in Harry Potter series? 
```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = Title)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()
# we can see that we have deviations at higher ranks.This is unusual, because the author uses  a lower percentage of the most common words
```

1. How does positive sentiment change over time?
```{r}
#Before I go ahead with the sentiment analysis I want to get an idea of the kind of words association with the NRC categories.

harrypotter_sentiment <- harrypotter_clean_tokens%>%
  inner_join(get_sentiments("nrc"), by = "word") %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(Title, Chapter,sentiment)%>%
  summarise(count=sum(score))
  

harrypotter_sentiment%>%
  filter(count>0)%>%
  ggplot( aes(x = Chapter, y = count)) + 
  geom_line(size = 1, color = brewer.pal(3, "Set1")[3]) +
  facet_grid(sentiment~.) +
  uniform_ggplot_theme() + 
  theme(
    axis.text.x = element_text(angle = 0, vjust = 0, hjust = 0),
    legend.position = "none") +
  labs(x = "Chapter", y = "Sentiment Value",
       title = "Sentiments Progression (Positive sentiments)")

```

2. How does negative sentiments changes over time
```{r}

harrypotter_sentiment%>%
  filter(count<0)%>%
  ggplot( aes(x = Chapter, y = count)) + 
  geom_line(size = 1, color = brewer.pal(3, "Set1")[1]) +
  facet_grid(sentiment~.) +
  uniform_ggplot_theme() + 
  theme(
    axis.text.x = element_text(angle = 0, vjust = 0, hjust = 0),
    legend.position = "none") +
  labs(x = "Chapter", y = "Sentiment Value",
       title = "Sentiments Progression (Negative sentiments)")
```


3. How does sentiment changes over time for each book? Which book has the highest variation?
```{r}


hp_df<- harrypotter_clean_tokens%>%
  inner_join(get_sentiments("bing"), by = "word") %>% # join sentiment
  count(Title, Chapter, sentiment, sort = TRUE)%>% # count sentiment
   spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

  ggplot(hp_df, aes(Chapter, sentiment, fill = Title))+
   geom_line(size = 1, color = brewer.pal(3, "Set1")[3]) +
  facet_wrap(~Title, ncol = 2, scales = "free_x")
  
  ggplot(hp_df, aes(Chapter, sentiment, colour = Title))+
   geom_line()+
    theme(
    legend.position ="bottom")
  # looks like Harry Potter and the Deathly Hallows has the highest sentimental variation
```

